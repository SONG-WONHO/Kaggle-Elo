{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사전작업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모듈 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T14:28:00.722197Z",
     "start_time": "2019-02-03T14:27:59.284113Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T14:28:00.727185Z",
     "start_time": "2019-02-03T14:28:00.723194Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T14:28:00.732171Z",
     "start_time": "2019-02-03T14:28:00.728181Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T14:28:00.973515Z",
     "start_time": "2019-02-03T14:28:00.970559Z"
    }
   },
   "outputs": [],
   "source": [
    "path = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T14:28:02.957799Z",
     "start_time": "2019-02-03T14:28:02.947825Z"
    }
   },
   "outputs": [],
   "source": [
    "# reduce memory\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T14:32:47.734475Z",
     "start_time": "2019-02-03T14:32:47.729459Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def mode(x):\n",
    "    return stats.mode(x)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T14:32:47.903133Z",
     "start_time": "2019-02-03T14:32:47.898145Z"
    }
   },
   "outputs": [],
   "source": [
    "def null_cnt(x):\n",
    "    return np.sum(x.isna() * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T14:32:48.057641Z",
     "start_time": "2019-02-03T14:32:48.053653Z"
    }
   },
   "outputs": [],
   "source": [
    "def over_550(data):\n",
    "    return len(data[data > 550])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T14:31:22.339828Z",
     "start_time": "2019-02-03T14:31:22.329856Z"
    }
   },
   "outputs": [],
   "source": [
    "# preprocessing train & test\n",
    "def train_test(num_rows=None):\n",
    "\n",
    "    # load csv\n",
    "    train = pd.read_csv(path + 'train.csv', index_col=['card_id'])\n",
    "    test = pd.read_csv(path + 'test.csv', index_col=['card_id'])\n",
    "    \n",
    "    # outlier\n",
    "    train['outliers'] = 0\n",
    "    train.loc[train['target'] < -30, 'outliers'] = 1\n",
    "\n",
    "    # set target as nan\n",
    "    test['target'] = np.nan\n",
    "\n",
    "    # merge\n",
    "    df = train.append(test)\n",
    "\n",
    "    del train, test\n",
    "    gc.collect()\n",
    "\n",
    "    # first_active_month\n",
    "    df['first_active'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['first_active_year'] = df['first_active'].dt.year\n",
    "    df['first_active_month'] = df['first_active'].dt.month\n",
    "    df['first_active_quarter'] = df['first_active'].dt.quarter\n",
    "    df['first_active_weekofyear'] = df['first_active'].dt.weekofyear\n",
    "    df['first_active_dayofweek'] = df['first_active'].dt.dayofweek\n",
    "    # 모든 데이터의 마지막 거래날짜가 2018년 4월 30일 23시 59분 59초\n",
    "    df['first_active_elapsed_time_from_trade'] = (datetime.datetime(2018, 4, 30, 23, 59, 59) - df['first_active']).dt.days\n",
    "    df['first_active_elapsed_time_from_today'] = (datetime.datetime.today() - df['first_active']).dt.days\n",
    "\n",
    "    df['days_feature1'] = df['feature_1'] * df['first_active_elapsed_time_from_trade']\n",
    "    df['days_feature2'] = df['feature_2'] * df['first_active_elapsed_time_from_trade'] \n",
    "    df['days_feature3'] = df['feature_3'] * df['first_active_elapsed_time_from_trade'] \n",
    "\n",
    "    df['days_feature1_ratio'] = df['feature_1'] / df['first_active_elapsed_time_from_trade']\n",
    "    df['days_feature2_ratio'] = df['feature_2'] / df['first_active_elapsed_time_from_trade']\n",
    "    df['days_feature3_ratio'] = df['feature_3'] / df['first_active_elapsed_time_from_trade']\n",
    "    \n",
    "    for f in ['feature_1','feature_2','feature_3']:\n",
    "        order_label = df.groupby([f])['outliers'].mean()\n",
    "        df[f] = df[f].map(order_label)\n",
    "\n",
    "    df['feature_sum'] = df['feature_1'] + df['feature_2'] + df['feature_3']\n",
    "    df['feature_mean'] = df['feature_sum'] / 3\n",
    "    df['feature_max'] = df[['feature_1', 'feature_2', 'feature_3']].max(axis=1)\n",
    "    df['feature_min'] = df[['feature_1', 'feature_2', 'feature_3']].min(axis=1)\n",
    "    df['feature_var'] = df[['feature_1', 'feature_2', 'feature_3']].std(axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### historical_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T14:50:10.658706Z",
     "start_time": "2019-02-03T14:50:10.618812Z"
    }
   },
   "outputs": [],
   "source": [
    "# preprocessing historical transactions\n",
    "def historical_transactions():\n",
    "    \n",
    "    # load csv\n",
    "    hist_df = pd.read_csv(path + 'historical_transactions.csv')\n",
    "\n",
    "    # fillna\n",
    "    hist_df['category_2'].fillna(1.0,inplace=True)\n",
    "    hist_df['category_3'].fillna('A',inplace=True)\n",
    "    hist_df['merchant_id'].fillna(np.nan,inplace=True)\n",
    "    hist_df['installments'].replace(-1, np.nan,inplace=True)\n",
    "    hist_df['installments'].replace(999, np.nan,inplace=True)\n",
    "\n",
    "    # trim\n",
    "    hist_df['purchase_amount_trim'] = hist_df['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
    "\n",
    "    # Y/N to 1/0\n",
    "    hist_df['authorized_flag'] = hist_df['authorized_flag'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "    hist_df['category_1'] = hist_df['category_1'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "    hist_df['category_3'] = hist_df['category_3'].map({'A':0, 'B':1, 'C':2})\n",
    "\n",
    "    # purchase date\n",
    "    hist_df['purchase_date'] = pd.to_datetime(hist_df['purchase_date'])\n",
    "    hist_df['purchase_year'] = hist_df['purchase_date'].dt.year\n",
    "    hist_df['purchase_month'] = hist_df['purchase_date'].dt.month\n",
    "    hist_df['purchase_day'] = hist_df['purchase_date'].dt.day\n",
    "    hist_df['purchase_hour'] = hist_df['purchase_date'].dt.hour\n",
    "    hist_df['purchase_dayofweek'] = hist_df['purchase_date'].dt.dayofweek\n",
    "    hist_df['purchase_weekofyear'] = hist_df['purchase_date'].dt.weekofyear\n",
    "    hist_df['purchase_weekend'] = (hist_df['purchase_date'].dt.weekday >=5).astype(int)\n",
    "\n",
    "    # additional features\n",
    "    hist_df['price'] = hist_df['purchase_amount'] / hist_df['installments']\n",
    "    \n",
    "    hist_df['month_diff'] = ((datetime.datetime(2018, 4, 30, 23, 59, 59) - hist_df['purchase_date']).dt.days)//30\n",
    "    hist_df['month_diff'] += hist_df['month_lag']\n",
    "    \n",
    "    hist_df['Christmas_Day_2017']=(pd.to_datetime('2017-12-25')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    hist_df['Mothers_Day_2017']=(pd.to_datetime('2017-06-04')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    hist_df['fathers_day_2017']=(pd.to_datetime('2017-08-13')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    hist_df['Children_day_2017']=(pd.to_datetime('2017-10-12')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    hist_df['Valentine_Day_2017']=(pd.to_datetime('2017-06-12')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    hist_df['Black_Friday_2017']=(pd.to_datetime('2017-11-24') - hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    hist_df['Mothers_Day_2018']=(pd.to_datetime('2018-05-13')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "    hist_df['duration'] = hist_df['purchase_amount'] * hist_df['month_diff']\n",
    "    hist_df['amount_month_ratio'] = hist_df['purchase_amount'] / hist_df['month_diff']\n",
    "\n",
    "    # reduce memory usage\n",
    "    hist_df = reduce_mem_usage(hist_df)\n",
    "    \n",
    "    aggs = {}\n",
    "    aggs['authorized_flag'] = [mode, 'sum', 'mean']\n",
    "    aggs['card_id'] = ['size']\n",
    "    aggs['city_id'] = [mode, 'nunique']\n",
    "    aggs['category_1'] = [mode, 'sum', 'mean']\n",
    "    aggs['installments'] = [mode, 'sum', 'mean', 'var', 'max', 'min', null_cnt]\n",
    "    aggs['category_3'] = [mode, 'mean']\n",
    "    aggs['merchant_category_id'] = [mode, 'nunique']\n",
    "    aggs['merchant_id'] = ['nunique']\n",
    "    aggs['month_lag'] = [mode, 'sum', 'mean', 'var', 'max', 'min', 'skew']\n",
    "    aggs['purchase_amount'] = ['sum', 'mean', 'var', 'max', 'min', 'skew', over_550]\n",
    "    aggs['category_2'] = [mode, 'mean']\n",
    "    aggs['state_id'] = [mode, 'nunique']\n",
    "    aggs['subsector_id'] = [mode, 'nunique']\n",
    "    aggs['purchase_amount_trim'] = ['sum', 'mean', 'var', 'max', 'min', 'skew']\n",
    "    aggs['purchase_year'] = [mode, 'nunique', 'mean', 'max', 'min']\n",
    "    aggs['purchase_month'] = [mode, 'nunique', 'mean', 'max', 'min']\n",
    "    aggs['purchase_day'] = [mode, 'nunique', 'mean', 'var', 'max', 'min', 'skew']\n",
    "    aggs['purchase_hour'] = [mode, 'nunique', 'mean', 'var', 'max', 'min', 'skew']\n",
    "    aggs['purchase_dayofweek'] = [mode, 'nunique', 'mean', 'max', 'min']\n",
    "    aggs['purchase_weekofyear'] = [mode, 'nunique', 'mean', 'max', 'min']\n",
    "    aggs['purchase_weekend'] = [mode, 'sum', 'mean']\n",
    "    aggs['price'] = ['sum','mean','max','min','var']\n",
    "    aggs['month_diff'] = ['max','min','mean','var','skew']\n",
    "    aggs['Christmas_Day_2017'] = ['mean']\n",
    "    aggs['Mothers_Day_2017'] = ['mean']\n",
    "    aggs['fathers_day_2017'] = ['mean']\n",
    "    aggs['Children_day_2017'] = ['mean']\n",
    "    aggs['Valentine_Day_2017'] = ['mean']\n",
    "    aggs['Black_Friday_2017'] = ['mean']\n",
    "    aggs['Mothers_Day_2018'] = ['mean']\n",
    "    aggs['duration']=['mean','min','max','var','skew']\n",
    "    aggs['amount_month_ratio']=['mean','min','max','var','skew']\n",
    "    \n",
    "    for col in ['category_2','category_3']:\n",
    "        hist_df[col+'_mean'] = hist_df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        hist_df[col+'_min'] = hist_df.groupby([col])['purchase_amount'].transform('min')\n",
    "        hist_df[col+'_max'] = hist_df.groupby([col])['purchase_amount'].transform('max')\n",
    "        hist_df[col+'_sum'] = hist_df.groupby([col])['purchase_amount'].transform('sum')\n",
    "        aggs[col+'_mean'] = ['mean']\n",
    "        aggs[col+'_min'] = ['mean']\n",
    "        aggs[col+'_max'] = ['mean']\n",
    "        aggs[col+'_sum'] = ['mean']\n",
    "    \n",
    "    hist_df = hist_df.groupby('card_id').agg(aggs)\n",
    "    \n",
    "    # change column name\n",
    "    hist_df.columns = pd.Index([e[0] + \"_\" + e[1] for e in hist_df.columns.tolist()])\n",
    "    hist_df.columns = ['hist_'+ c for c in hist_df.columns]\n",
    "\n",
    "    hist_df['hist_purchase_date_diff'] = (hist_df['hist_purchase_date_max']-hist_df['hist_purchase_date_min']).dt.days\n",
    "    hist_df['hist_purchase_date_average'] = hist_df['hist_purchase_date_diff']/hist_df['hist_card_id_size']\n",
    "    hist_df['hist_purchase_date_uptonow'] = (datetime.datetime.today()-hist_df['hist_purchase_date_max']).dt.days\n",
    "    hist_df['hist_purchase_date_uptomin'] = (datetime.datetime.today()-hist_df['hist_purchase_date_min']).dt.days\n",
    "\n",
    "    # reduce memory usage\n",
    "    hist_df = reduce_mem_usage(hist_df)\n",
    "\n",
    "    return hist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new_merchant_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T15:15:45.377187Z",
     "start_time": "2019-02-03T15:13:08.717Z"
    }
   },
   "outputs": [],
   "source": [
    "# preprocessing new_merchant_transactions\n",
    "def new_merchant_transactions():\n",
    "    \n",
    "    # load csv\n",
    "    new_merchant_df = pd.read_csv(path + 'new_merchant_transactions.csv')\n",
    "\n",
    "    # fillna\n",
    "    new_merchant_df['category_2'].fillna(1.0,inplace=True)\n",
    "    new_merchant_df['category_3'].fillna('A',inplace=True)\n",
    "    new_merchant_df['merchant_id'].fillna(np.nan,inplace=True)\n",
    "    new_merchant_df['installments'].replace(-1, np.nan,inplace=True)\n",
    "    new_merchant_df['installments'].replace(999, np.nan,inplace=True)\n",
    "\n",
    "    # trim\n",
    "    new_merchant_df['purchase_amount_trim'] = new_merchant_df['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
    "\n",
    "    # Y/N to 1/0\n",
    "    new_merchant_df['authorized_flag'] = new_merchant_df['authorized_flag'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "    new_merchant_df['category_1'] = new_merchant_df['category_1'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "    new_merchant_df['category_3'] = new_merchant_df['category_3'].map({'A':0, 'B':1, 'C':2})\n",
    "\n",
    "    # purchase date\n",
    "    new_merchant_df['purchase_date'] = pd.to_datetime(new_merchant_df['purchase_date'])\n",
    "    new_merchant_df['purchase_year'] = new_merchant_df['purchase_date'].dt.year\n",
    "    new_merchant_df['purchase_month'] = new_merchant_df['purchase_date'].dt.month\n",
    "    new_merchant_df['purchase_day'] = new_merchant_df['purchase_date'].dt.day\n",
    "    new_merchant_df['purchase_hour'] = new_merchant_df['purchase_date'].dt.hour\n",
    "    new_merchant_df['purchase_dayofweek'] = new_merchant_df['purchase_date'].dt.dayofweek\n",
    "    new_merchant_df['purchase_weekofyear'] = new_merchant_df['purchase_date'].dt.weekofyear\n",
    "    new_merchant_df['purchase_weekend'] = (new_merchant_df['purchase_date'].dt.weekday >=5).astype(int)\n",
    "\n",
    "    # additional features\n",
    "    new_merchant_df['price'] = new_merchant_df['purchase_amount'] / new_merchant_df['installments']\n",
    "    \n",
    "    new_merchant_df['month_diff'] = ((datetime.datetime(2018, 4, 30, 23, 59, 59) - new_merchant_df['purchase_date']).dt.days)//30\n",
    "    new_merchant_df['month_diff'] += new_merchant_df['month_lag']\n",
    "\n",
    "    new_merchant_df['Christmas_Day_2017']=(pd.to_datetime('2017-12-25')-new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    new_merchant_df['Children_day_2017']=(pd.to_datetime('2017-10-12')-new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    new_merchant_df['Black_Friday_2017']=(pd.to_datetime('2017-11-24') - new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    new_merchant_df['Mothers_Day_2018']=(pd.to_datetime('2018-05-13')-new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "                                     \n",
    "    new_merchant_df['duration'] = new_merchant_df['purchase_amount'] * new_merchant_df['month_diff']\n",
    "    new_merchant_df['amount_month_ratio'] = new_merchant_df['purchase_amount'] / new_merchant_df['month_diff']\n",
    "\n",
    "    # reduce memory usage\n",
    "    new_merchant_df = reduce_mem_usage(new_merchant_df)\n",
    "    \n",
    "    aggs = {}\n",
    "    aggs['authorized_flag'] = [mode, 'sum', 'mean']\n",
    "    aggs['card_id'] = ['size']\n",
    "    aggs['city_id'] = [mode, 'nunique']\n",
    "    aggs['category_1'] = [mode, 'sum', 'mean']\n",
    "    aggs['installments'] = [mode, 'sum', 'mean', 'var', 'max', 'min', null_cnt]\n",
    "    aggs['category_3'] = [mode, 'mean']\n",
    "    aggs['merchant_category_id'] = [mode, 'nunique']\n",
    "    aggs['merchant_id'] = ['nunique']\n",
    "    aggs['month_lag'] = [mode, 'sum', 'mean', 'var', 'max', 'min', 'skew']\n",
    "    aggs['purchase_amount'] = ['sum', 'mean', 'var', 'max', 'min', 'skew', over_550]\n",
    "    aggs['category_2'] = [mode, 'mean']\n",
    "    aggs['state_id'] = [mode, 'nunique']\n",
    "    aggs['subsector_id'] = [mode, 'nunique']\n",
    "    aggs['purchase_amount_trim'] = ['sum', 'mean', 'var', 'max', 'min', 'skew']\n",
    "    aggs['purchase_year'] = [mode, 'nunique', 'mean', 'max', 'min']\n",
    "    aggs['purchase_month'] = [mode, 'nunique', 'mean', 'max', 'min']\n",
    "    aggs['purchase_day'] = [mode, 'nunique', 'mean', 'var', 'max', 'min', 'skew']\n",
    "    aggs['purchase_hour'] = [mode, 'nunique', 'mean', 'var', 'max', 'min', 'skew']\n",
    "    aggs['purchase_dayofweek'] = [mode, 'nunique', 'mean', 'max', 'min']\n",
    "    aggs['purchase_weekofyear'] = [mode, 'nunique', 'mean', 'max', 'min']\n",
    "    aggs['purchase_weekend'] = [mode, 'sum', 'mean']\n",
    "    aggs['price'] = ['sum','mean','max','min','var']\n",
    "    aggs['month_diff'] = ['max','min','mean','var','skew']\n",
    "    aggs['Christmas_Day_2017'] = ['mean']\n",
    "    aggs['Children_day_2017'] = ['mean']\n",
    "    aggs['Black_Friday_2017'] = ['mean']\n",
    "    aggs['Mothers_Day_2018'] = ['mean']\n",
    "    aggs['duration']=['mean','min','max','var','skew']\n",
    "    aggs['amount_month_ratio']=['mean','min','max','var','skew']\n",
    "\n",
    "    for col in ['category_2','category_3']:\n",
    "        new_merchant_df[col+'_mean'] = new_merchant_df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        new_merchant_df[col+'_min'] = new_merchant_df.groupby([col])['purchase_amount'].transform('min')\n",
    "        new_merchant_df[col+'_max'] = new_merchant_df.groupby([col])['purchase_amount'].transform('max')\n",
    "        new_merchant_df[col+'_sum'] = new_merchant_df.groupby([col])['purchase_amount'].transform('sum')\n",
    "        aggs[col+'_mean'] = ['mean']\n",
    "\n",
    "    new_merchant_df = new_merchant_df.groupby('card_id').agg(aggs)\n",
    "\n",
    "    # change column name\n",
    "    new_merchant_df.columns = pd.Index([e[0] + \"_\" + e[1] for e in new_merchant_df.columns.tolist()])\n",
    "    new_merchant_df.columns = ['new_'+ c for c in new_merchant_df.columns]\n",
    "\n",
    "    new_merchant_df['new_purchase_date_diff'] = (new_merchant_df['new_purchase_date_max']-new_merchant_df['new_purchase_date_min']).dt.days\n",
    "    new_merchant_df['new_purchase_date_average'] = new_merchant_df['new_purchase_date_diff']/new_merchant_df['new_card_id_size']\n",
    "    new_merchant_df['new_purchase_date_uptonow'] = (datetime.datetime.today()-new_merchant_df['new_purchase_date_max']).dt.days\n",
    "    new_merchant_df['new_purchase_date_uptomin'] = (datetime.datetime.today()-new_merchant_df['new_purchase_date_min']).dt.days\n",
    "\n",
    "    # reduce memory usage\n",
    "    new_merchant_df = reduce_mem_usage(new_merchant_df)\n",
    "\n",
    "    return new_merchant_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### additional_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T14:14:54.860003Z",
     "start_time": "2019-02-03T14:14:54.678241Z"
    }
   },
   "outputs": [],
   "source": [
    "# additional features\n",
    "def additional_features(df):\n",
    "    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['hist_last_buy'] = (df['hist_purchase_date_max'] - df['first_active_month']).dt.days\n",
    "    df['new_first_buy'] = (df['new_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['new_last_buy'] = (df['new_purchase_date_max'] - df['first_active_month']).dt.days\n",
    "\n",
    "    date_features=['hist_purchase_date_max','hist_purchase_date_min',\n",
    "                   'new_purchase_date_max', 'new_purchase_date_min']\n",
    "\n",
    "    for f in date_features:\n",
    "        df[f] = df[f].astype(np.int64) * 1e-9\n",
    "\n",
    "    df['card_id_total'] = df['new_card_id_size']+df['hist_card_id_size']\n",
    "    df['card_id_cnt_total'] = df['new_card_id_count']+df['hist_card_id_count']\n",
    "    df['card_id_cnt_ratio'] = df['new_card_id_count']/df['hist_card_id_count']\n",
    "    df['purchase_amount_total'] = df['new_purchase_amount_sum']+df['hist_purchase_amount_sum']\n",
    "    df['purchase_amount_mean'] = df['new_purchase_amount_mean']+df['hist_purchase_amount_mean']\n",
    "    df['purchase_amount_max'] = df['new_purchase_amount_max']+df['hist_purchase_amount_max']\n",
    "    df['purchase_amount_min'] = df['new_purchase_amount_min']+df['hist_purchase_amount_min']\n",
    "    df['purchase_amount_ratio'] = df['new_purchase_amount_sum']/df['hist_purchase_amount_sum']\n",
    "    df['month_diff_mean'] = df['new_month_diff_mean']+df['hist_month_diff_mean']\n",
    "    df['month_diff_ratio'] = df['new_month_diff_mean']/df['hist_month_diff_mean']\n",
    "    df['month_lag_mean'] = df['new_month_lag_mean']+df['hist_month_lag_mean']\n",
    "    df['month_lag_max'] = df['new_month_lag_max']+df['hist_month_lag_max']\n",
    "    df['month_lag_min'] = df['new_month_lag_min']+df['hist_month_lag_min']\n",
    "    df['category_1_mean'] = df['new_category_1_mean']+df['hist_category_1_mean']\n",
    "    df['installments_total'] = df['new_installments_sum']+df['hist_installments_sum']\n",
    "    df['installments_mean'] = df['new_installments_mean']+df['hist_installments_mean']\n",
    "    df['installments_max'] = df['new_installments_max']+df['hist_installments_max']\n",
    "    df['installments_ratio'] = df['new_installments_sum']/df['hist_installments_sum']\n",
    "    df['price_total'] = df['purchase_amount_total'] / df['installments_total']\n",
    "    df['price_mean'] = df['purchase_amount_mean'] / df['installments_mean']\n",
    "    df['price_max'] = df['purchase_amount_max'] / df['installments_max']\n",
    "    df['duration_mean'] = df['new_duration_mean']+df['hist_duration_mean']\n",
    "    df['duration_min'] = df['new_duration_min']+df['hist_duration_min']\n",
    "    df['duration_max'] = df['new_duration_max']+df['hist_duration_max']\n",
    "    df['amount_month_ratio_mean']=df['new_amount_month_ratio_mean']+df['hist_amount_month_ratio_mean']\n",
    "    df['amount_month_ratio_min']=df['new_amount_month_ratio_min']+df['hist_amount_month_ratio_min']\n",
    "    df['amount_month_ratio_max']=df['new_amount_month_ratio_max']+df['hist_amount_month_ratio_max']\n",
    "    df['new_CLV'] = df['new_card_id_count'] * df['new_purchase_amount_sum'] / df['new_month_diff_mean']\n",
    "    df['hist_CLV'] = df['hist_card_id_count'] * df['hist_purchase_amount_sum'] / df['hist_month_diff_mean']\n",
    "    df['CLV_ratio'] = df['new_CLV'] / df['hist_CLV']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T14:31:34.674326Z",
     "start_time": "2019-02-03T14:31:33.640929Z"
    }
   },
   "outputs": [],
   "source": [
    "df = train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T15:15:45.374193Z",
     "start_time": "2019-02-03T14:50:32.686150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 2110.04 MB\n",
      "Decreased by 69.8%\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'hist_purchase_date_max'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3077\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3078\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3079\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'hist_purchase_date_max'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-26fa2c9bf9e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistorical_transactions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-9a539117b4ef>\u001b[0m in \u001b[0;36mhistorical_transactions\u001b[1;34m()\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[0mhist_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'hist_'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhist_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[0mhist_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hist_purchase_date_diff'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhist_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hist_purchase_date_max'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mhist_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hist_purchase_date_min'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m     \u001b[0mhist_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hist_purchase_date_average'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhist_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hist_purchase_date_diff'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mhist_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hist_card_id_size'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[0mhist_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hist_purchase_date_uptonow'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoday\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mhist_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hist_purchase_date_max'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2686\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2687\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2688\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2690\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2693\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2694\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2695\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2697\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2488\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2489\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   4113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4114\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4115\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4116\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3078\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'hist_purchase_date_max'"
     ]
    }
   ],
   "source": [
    "hist = historical_transactions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T15:15:45.378183Z",
     "start_time": "2019-02-03T15:13:25.429Z"
    }
   },
   "outputs": [],
   "source": [
    "new_hist = new_merchant_transactions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T15:15:45.380177Z",
     "start_time": "2019-02-03T15:14:45.580Z"
    }
   },
   "outputs": [],
   "source": [
    "result = pd.merge(df, hist, on='card_id', how='outer')\n",
    "result = pd.merge(result, new_hist, on='card_id', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = additional_features(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
